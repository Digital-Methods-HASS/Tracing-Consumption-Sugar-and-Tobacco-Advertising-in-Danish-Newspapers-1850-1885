---
title: "Final project tobak"
author: "Kamilla Dalgaard, Joe Timmins, Asger Tonsberg and Christian Bæk"
date: "2025-05-08"
output: html_document
---

Thanks to Max Odsbjerg Pedersen for his code and script - And for his help

We have used the Used the website: [SwaggerUI](http://labs.statsbiblioteket.dk/labsapi/api//api-docs?url=/labsapi/api/openapi.yaml)
The searchcode used in Mediestream is then inserted in the Swagger Interfacet under GET/aviser/export/fields. 
After loading the website provides a URL, that we can now use for our coding. 
The API delivers data from the Royal Danish Library's newspaper collection. 
 

#Libraries 
To start with we will download and open the packages we'll be using for this project:
```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(urltools)
```

# Loading data from newspapers from 1850-1885 that have adverts about tobacco in them.

The dataset is loaded into R via a retrieve link from the API. This link is created by the Swagger UI:

```{r}
Tobak <- "https://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=Tobak%20AND%20lplace%3AK%C3%B8benhavn%20AND%20py%3A%5B1850%20TO%201885%5D&fields=link&fields=recordID&fields=timestamp&fields=pwa&fields=cer&fields=fulltext_org&fields=pageUUID&fields=editionUUID&fields=titleUUID&fields=editionId&fields=familyId&fields=newspaper_page&fields=newspaper_edition&fields=lplace&fields=location_name&fields=location_coordinates&max=-1&structure=header&structure=content&format=CSV"
```

#Decode the URL
We will use the url_decode function to make the URL more readable:

```{r}
url_decode(Tobak)
```

From this we are able to see the query/searchcode we used in Mediestream.

Our next step is to load our data into R using the read_csv function: 

```{r}
Tobak_1850_1885 <- read_csv(Tobak)
glimpse(Tobak_1850_1885)
write.csv2(Tobak_1850_1885, "Data/tobacco1850_1885")
```

## Analysing our Tobacco data 

By doing this we can see as we targeted the news papers are from Copenhagen and we have 19922 hits. This gives us some metadata about the newspapers. 

```{r}
Tobak_1850_1885 %>% 
  count(lplace, sort = TRUE)
```
We also have meta data on which newspapers the articles derives from: 

```{r}
Tobak_1850_1885 %>% 
  count(familyId, sort = TRUE)
```

# Text Mining: 
We will be using the tidy text package with the unnest_tokens function to text mine our data.
The unnest_tokens package takes texts and breaks it into individual words. In this way, there will be just one word per row in the dataset. this makes it possible for us to target the word "tobak". This helps us clean it and now our data isn't messy any more and now we have Tobak_tidy.

```{r}
Tobak_tidy <- Tobak_1850_1885 %>% 
  unnest_tokens(word, fulltext_org)
```

To clean the data more we loaded in a stopword list which will sort out all the most common words such as "at", "det" etc. We got the stopwordlist from Max Odbjerg Pedersen:

```{r}
stopord_1800 <- read_csv("https://gist.githubusercontent.com/maxodsbjerg/1537cf14c3d46b3d30caa5d99f8758e9/raw/9f044a38505334f035be111c9a3f654a24418f6d/stopord_18_clean.csv")
```

# How frequent is the word "Tobak"
Using "anti_join" before "count" we can sort out the stop words. We did this to see how frequent the word "tobak" came up in the newspapers: 

```{r}
Tobak_tidy %>% 
  anti_join(stopord_1800) %>% 
  count(word, sort = TRUE)
```



As we can see "Tobak" is quite frequent in the newspaper, actually number 1, which could indicate it was advertised a lot

#Filter out other words 

```{r}
#here we will filter out all other words so we're only left with "tobak" so we can use it to make a
# graph that shows the frequency of the word tobacco over time.

Tobak_most_used <- Tobak_tidy %>% 
  anti_join(stopord_1800) %>% 
  count(word, sort = TRUE)

most_used_word <- Tobak_most_used %>%
  slice(1)

print(most_used_word)
```

#Making the graph for Tobacco consumption 

```{r}
#to make the graph we dowload the necesary packages dplyr will help us filter the word "tobak" and 
# ggplot2 will make it possible for us to make graphs and add trend line. 
library(dplyr)
library(ggplot2)

# Here we will filter for the word "tobak" and count per year
tobak_over_time <- Tobak_tidy %>%
  filter(word == "tobak") %>%
  count(timestamp)

# Code for the Graph
tobak_plot <- ggplot(tobak_over_time, aes(x = timestamp, y = n)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = 'The Development of Tobacco ads during 1850-1885',
    x = "Year",
    y = "Hits"
  ) +
  theme_minimal() 

print(tobak_plot)
```

#Picture 
```{r}
#code for generating the picture
ggsave("tobak_over_time.png", plot = tobak_plot, width = 8, height = 6, dpi = 300)
```

We wanted a graph with a trendline to see the general development 

```{r}
#code for graph with trendline
tobak_plot_with_line <- ggplot(tobak_over_time, aes(x = timestamp, y = n)) +
  geom_line(color = "blue", size = 1) +
  geom_smooth(method = "loess", se = FALSE, color = "darkgreen", size = 1) +  # Tendenslinje
  labs(
    title = 'The Development of Tobacco Ads During 1850–1885',
    x = "Year",
    y = "Hits"
  ) +
  theme_minimal()

print(tobak_plot_with_line)
```

#Another Picture 
```{r}
ggsave("tobak_over_time_with_line.png", plot = tobak_plot_with_line, width = 8, height = 6, dpi = 300)
```

