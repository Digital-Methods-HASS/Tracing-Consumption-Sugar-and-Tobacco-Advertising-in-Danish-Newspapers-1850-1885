---
title: "Final project Sukker"
author: "Kamilla Dalgaard, Joe Timmins, Asger Tonsberg og Christian Bæk"
date: "2025-05-23"
output: html_document
---

Thanks to Max Odsbjerg Pedersen for his code and script - And for his help

We have used the Used the website: [SwaggerUI](http://labs.statsbiblioteket.dk/labsapi/api//api-docs?url=/labsapi/api/openapi.yaml)
The searchcode used in Mediestream is then inserted in the Swagger Interfacet under GET/aviser/export/fields. 
After loading the website provides a URL, that we can now use for our coding. 
The API delivers data from the Royal Danish Library's newspaper collection. 

#Libraries 
To start with we will download and open the packages we'll be using for this project:
```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(urltools)
```

# Loading data from newspapers from 1850-1885 that have adverts about sugar in them.

The dataset is loaded into R via a retrieve link from the API. This link is created by the Swagger UI:
```{r}
Sukker <- "https://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=Sukker%20AND%20lplace%3AK%C3%B8benhavn%20py%3A%5B1850%20TO%201885%5D&fields=link&fields=recordID&fields=timestamp&fields=pwa&fields=cer&fields=fulltext_org&fields=pageUUID&fields=editionUUID&fields=titleUUID&fields=editionId&fields=familyId&fields=newspaper_page&fields=newspaper_edition&fields=lplace&fields=location_name&fields=location_coordinates&max=-1&structure=header&structure=content&format=CSV"
```

#Decode the URL
We will use the url_decode function to make the URL more readable:
```{r}
url_decode(Sukker)
```


From this we are able to see the query/searchcode we used in Mediestream.

Next step is to load our data into R using the read_csv function and  we will also create a csv file for our dataset: 

```{r}
Sukker_1850_1885 <- read_csv(Sukker)
glimpse(Sukker_1850_1885)
write.csv2(Sukker_1850_1885, "Data/sugar1850_85")
```

## Analysing our Sugar data

By doing this we can see as we targeted the news papers are from Copenhagen and we have 82449 hits. This gives us some metadata about the newspapers. 

```{r}
Sukker_1850_1885 %>% 
  count(lplace, sort = TRUE)
```
We also have meta data on which newspapers the articles derives from: 

```{r}
Sukker_1850_1885 %>% 
  count(familyId, sort = TRUE)
```

# Text Mining: 
Here we will use the tidy text package with the unnest_tokens function to text mine our data.
The unnest_tokens package takes texts and breaks it into individual words. In this way, there will be just one word per row in the dataset. this makes it possible for us to target the word "sukker". This helps us clean it and now our data isn't messy any more and now we have Sukker_tidy

```{r}
Sukker_tidy <- Sukker_1850_1885 %>% 
  unnest_tokens(word, fulltext_org)
```

To clean the data more we loaded in a stopword list which will sort out all the most common words such as "at", "det" etc. We got the stopwordlist from Max Odbjerg Pedersen:

```{r}
stopord_1800 <- read_csv("https://gist.githubusercontent.com/maxodsbjerg/1537cf14c3d46b3d30caa5d99f8758e9/raw/9f044a38505334f035be111c9a3f654a24418f6d/stopord_18_clean.csv")
```

# How frequent is the word Sukker
Using "anti_join" before "count" we can sort out the stop words. We did this to see how frequent the word "sukker" came up in the newspapers: 

```{r}
Sukker_tidy %>% 
  anti_join(stopord_1800) %>% 
  count(word, sort = TRUE)
```

As we can see sukker is quite frequent in the newspaper which could indicate it was advertised a lot

#Filter out other words 


```{r}
Sukker_most_used <- Sukker_tidy %>% 
  anti_join(stopord_1800) %>% 
  count(word, sort = TRUE)

most_used_word <- Sukker_most_used %>%
  slice(2)

print(most_used_word)
```

#Making the graph for Sugar consumption 

```{r}
#to make the graph we dowload the necesary packages dplyr will help us filter the word "tobak" and 
# ggplot2 will make it possible for us to make graphs and add trend line. 
library(dplyr)
library(ggplot2)

# Here we will filter for the word "tobak" and count per year
Sukker_over_time <- Sukker_tidy %>%
  filter(word == "sukker") %>%
  count(timestamp)

# Code for the Graph
sukker_plot <- ggplot(Sukker_over_time, aes(x = timestamp, y = n)) +
  geom_line(color = "magenta", size = 1) +
  labs(
    title = 'The Development of sugar ads during 1850-1885',
    x = "Year",
    y = "Hits"
  ) +  theme_minimal()

print(sukker_plot)
```

#Picture 
```{r}
#code for generating the picture
ggsave("sukker_over_time.png", plot = sukker_plot, width = 8, height = 6, dpi = 300)
```

We wanted a graph with a trendline to see the general development 

```{r}
#code for graph with trendline
sukker_plot_with_line <- ggplot(Sukker_over_time, aes(x = timestamp, y = n)) +
  geom_line(color = "magenta", size = 1) +
  geom_smooth(method = "loess", se = FALSE, color = "darkgreen", size = 1) +  # Trend line
  labs(
    title = 'The Development of Sugar Ads During 1850–1885',
    x = "Year",
    y = "Hits"
  ) +
  theme_minimal()

print(sukker_plot_with_line)
```

#Another Picture 
```{r}
ggsave("sukker_over_time_with_line.png", plot = sukker_plot_with_line, width = 8, height = 6, dpi = 300)
```

